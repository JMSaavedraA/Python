{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centro de Investigación en Matemáticas\n",
    "# Inteligencia Artificial\n",
    "## Tarea 4. Predicción de profondidad por CNN\n",
    "### José Miguel Saavedra Aguilar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos los paquetes que utilizaremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la VGG16 preentrenada en ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base=None\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(224, 224, 3))\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta tarea se realizó en Docker con tensorflow 2.16.1 y soporte para CUDA. El problema es que CUDA no funciona en Jupyter, por lo que aquí se encuentra el código que se debe correr en un Contenedor de Docker con la imagen de tensorflow-gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for input and output directories\n",
    "base_dir = '/app/data_depth_selection/depth_selection'\n",
    "train_dir = os.path.join(base_dir, 'test_depth_completion_anonymous')\n",
    "val_dir = os.path.join(base_dir, 'val_selection_cropped')\n",
    "\n",
    "# Create ImageDataGenerator for input images with preprocessing function\n",
    "input_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, rescale=1./255.0)\n",
    "# Create ImageDataGenerator for output images with resizing (if needed)\n",
    "output_datagen = ImageDataGenerator(rescale=1./255.0)\n",
    "# Define batch size\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos crear generadores auxiliares para tener la estructura deseada, input imagen rgb y output depth b/n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train_generator = input_datagen.flow_from_directory(\n",
    "    os.path.join(train_dir, 'image'),\n",
    "    target_size=(224, 224),  # Assuming VGG16 input size\n",
    "    batch_size=batch_size,\n",
    "    shuffle    = False,\n",
    "    class_mode=None  # Since this is an autoencoder, we don't need labels\n",
    ")\n",
    "\n",
    "depth_train_generator = output_datagen.flow_from_directory(\n",
    "    os.path.join(train_dir, 'velodyne_raw'),\n",
    "    target_size=(224, 224),  # Assuming VGG16 input size\n",
    "    batch_size=batch_size,\n",
    "    shuffle    = False,\n",
    "    class_mode=None  # Since this is an autoencoder, we don't need labels\n",
    ")\n",
    "\n",
    "image_val_generator = input_datagen.flow_from_directory(\n",
    "    os.path.join(val_dir, 'image'),\n",
    "    target_size=(224, 224),  # Assuming VGG16 input size\n",
    "    batch_size=batch_size,\n",
    "    shuffle    = False,\n",
    "    class_mode=None  # Since this is an autoencoder, we don't need labels\n",
    ")\n",
    "\n",
    "depth_val_generator = output_datagen.flow_from_directory(\n",
    "    os.path.join(val_dir, 'velodyne_raw'),\n",
    "    target_size=(224, 224),  # Assuming VGG16 input size\n",
    "    batch_size=batch_size,\n",
    "    shuffle    = False,\n",
    "    class_mode=None  # Since this is an autoencoder, we don't need labels\n",
    ")\n",
    "\n",
    "# Custom generator to combine two generators into one\n",
    "\n",
    "class JoinedGen(tf.keras.utils.Sequence):\n",
    "    def __init__(self, input_gen1, input_gen2):\n",
    "        self.gen1 = input_gen1\n",
    "        self.gen2 = input_gen2\n",
    "        assert len(input_gen1) == len(input_gen2), \"Input generators must have the same length.\"\n",
    "    def __len__(self):\n",
    "        return min(len(self.gen1), len(self.gen2))\n",
    "    def __getitem__(self, i):\n",
    "        x = self.gen1.__getitem__(i)\n",
    "        y = self.gen2.__getitem__(i)\n",
    "        return x, y\n",
    "    def on_epoch_end(self):\n",
    "        if hasattr(self.gen1, 'on_epoch_end'):\n",
    "            self.gen1.on_epoch_end()\n",
    "        if hasattr(self.gen2, 'on_epoch_end'):\n",
    "            self.gen2.on_epoch_end()\n",
    "\n",
    "\n",
    "train_generator = JoinedGen(image_train_generator, depth_train_generator)\n",
    "val_generator = JoinedGen(image_val_generator, depth_val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fijamos ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos ahora nuestro autoencoder. Añadimos varias capas de convolución transpuestas, de upscaling y densas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the decoder part of the autoencoder\n",
    "autoencoder = models.Sequential()\n",
    "autoencoder.add(conv_base)\n",
    "autoencoder.add(layers.UpSampling2D((8, 8)))\n",
    "autoencoder.add(layers.Conv2DTranspose(256, (3, 3), activation='sigmoid', padding='same'))\n",
    "autoencoder.add(layers.Conv2DTranspose(256, (3, 3), activation='sigmoid', padding='same'))\n",
    "autoencoder.add(layers.Dense(128, activation='sigmoid'))\n",
    "autoencoder.add(layers.UpSampling2D((2, 2)))\n",
    "autoencoder.add(layers.Conv2DTranspose(128, (3, 3), activation='sigmoid', padding='same'))\n",
    "autoencoder.add(layers.Conv2DTranspose(128, (3, 3), activation='sigmoid', padding='same'))\n",
    "autoencoder.add(layers.Dense(64, activation='sigmoid'))\n",
    "autoencoder.add(layers.UpSampling2D((2, 2)))\n",
    "autoencoder.add(layers.Conv2DTranspose(64, (3, 3), activation='sigmoid', padding='same'))\n",
    "autoencoder.add(layers.Conv2DTranspose(64, (3, 3), activation='sigmoid', padding='same'))\n",
    "autoencoder.add(layers.Dense(3, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He probado con diferentes pérdidas, diferentes estructuras de la red, optimizadores, etc. y en mi opinión ninguna funciona de forma satisfactoria, pues las imágenes se vuelven a negro casi en su totalidad. Utilizo la función de costo SSIM que debe comparar las características de ambas imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SSIM loss\n",
    "def ssim_loss(y_true, y_pred):\n",
    "    return 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=255.0))\n",
    "\n",
    "# Compile the model with SSIM loss\n",
    "autoencoder.compile(optimizer='adam', loss = [ssim_loss])\n",
    "\n",
    "# Compile the model with Cosine Similarity loss\n",
    "#autoencoder.compile(optimizer='adam', loss='cosine_similarity')\n",
    "\n",
    "# Compile the model with Mean Squared Error loss\n",
    "#autoencoder.compile(optimizer='sgd', loss='mse')  # Mean Squared Error loss for image reconstruction\n",
    "\n",
    "# Compile the model with Mean Absolute Error loss\n",
    "#autoencoder.compile(optimizer='adam', loss='mae')  # Mean Squared Error loss for image reconstruction\n",
    "\n",
    "autoencoder.build((None, 224, 224, 3))\n",
    "\n",
    "# Print summary\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos con 20 épocas. Lo hice hasta por 100 y no cambia, de hecho, casi a las 3 épocas queda constante. Finalmente guardamos para poder extraer el modelo y cargarlo después."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "# Train the autoencoder\n",
    "steps_per_epoch = len(train_generator) # Number of batches per epoch\n",
    "validation_steps = len(val_generator)  # Number of batches for validation\n",
    "\n",
    "autoencoder.fit(train_generator, \n",
    "                steps_per_epoch=steps_per_epoch, \n",
    "                epochs=num_epochs, \n",
    "                validation_data=val_generator, \n",
    "                validation_steps=validation_steps)\n",
    "\n",
    "# Save the trained model\n",
    "autoencoder.save('/app/convautoencoder.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vemos si la red aprendió lo que queremos. Tomamos una imagen del conjunto de validación y la pasamos por la red. La guardamos para poder verla después. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Load a sample image\n",
    "sample_image_path = '/app/data_depth_selection/depth_selection/val_selection_cropped/image/1/2011_09_26_drive_0005_sync_image_0000000092_image_02.png'\n",
    "sample_image = Image.open(sample_image_path)\n",
    "sample_image = sample_image.resize((224, 224))  # Resize to match model input size\n",
    "sample_image_array = np.array(sample_image)\n",
    "\n",
    "# Preprocess the sample image using input_datagen\n",
    "preprocessed_sample_image = preprocess_input(sample_image_array)\n",
    "\n",
    "# Expand dimensions to match model input shape\n",
    "preprocessed_sample_image = np.expand_dims(preprocessed_sample_image, axis=0)\n",
    "\n",
    "# Predict using the autoencoder model\n",
    "predicted_output = autoencoder.predict(preprocessed_sample_image)\n",
    "\n",
    "# Denormalize the output (if needed)\n",
    "predicted_output = np.clip(predicted_output * 255.0, 0, 255).astype(np.uint8)\n",
    "\n",
    "# Convert the predicted output to an image\n",
    "predicted_output_image = Image.fromarray(predicted_output[0]).resize((1216,352))\n",
    "\n",
    "# Save the predicted output image\n",
    "predicted_output_image.save(\"predicted_output.png\")\n",
    "\n",
    "print(\"Predicted output image saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se mencionó antes, esta tarea se corrió en Docker, y la imagen es obtenida desde ahí. Se puede cargar el modelo autoencoder.keras para verificar la red. Se incluye el script de Python que se corrió en docker"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
