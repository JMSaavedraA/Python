{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e7acce3",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center;\">\n",
    "    <div style=\"display: flex; flex-direction: column; align-items: center;\">\n",
    "        <h1 style=\"text-align: right;\">Centro de Investigación en Matemáticas, A.C.</h1>\n",
    "        <h2 style=\"text-align: right;\">Reconocimiento de Patrones</h2>\n",
    "        <h2 style=\"text-align: right;\">Jose Miguel Saavedra Aguilar</h2>\n",
    "        <h3 style=\"text-align: right;\">Examen 2. Ejercicio 2</h3>\n",
    "    </div>\n",
    "    <img src=\"Figures\\logoCIMAT11.png\" alt=\"CIMAT Logo\" width=\"200\" style=\"float: left;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e4f376",
   "metadata": {},
   "source": [
    "Para el segundo ejercicio del examen, se implementará un algoritmo Expectation-Maximization (EM) para ajustar una mezcla de $k$ distribuciones gaussianas a un conjunto de datos.\n",
    "\n",
    "Para este fin, nos basamos en el libro de Bishop, \"Pattern Recognition and Machine Learning\" del 2009, pues en clase solo estudiamos el algoritmo para variables reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73cfad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for image processing, numerical operations, and statistics\n",
    "from PIL import Image  # For image loading and manipulation\n",
    "import numpy as np     # For numerical operations and array handling\n",
    "from scipy.stats import multivariate_normal  # For multivariate Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d86a27e",
   "metadata": {},
   "source": [
    "Segun Bishop, para maximizar la verosimilitud de los datos, se deriva la función de log-verosimilitud con respecto a las medias e igualamos a cero, obteniendo la siguiente expresión:\n",
    "$$\n",
    "\\gamma(z_{nk}) = \\sum_j \\pi_j \\mathcal{N}(x_n | \\mu_j, \\Sigma_j)\\\\\n",
    "0 = - \\sum_{n=1}^N \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, {\\Sigma} _k)}{\\gamma(z_{nk})} \\Sigma_k (x_n - \\mu_k).\n",
    "$$\n",
    "A $\\gamma(z_{nk})$ se le conoce como la responsabilidad, o la probabilidad posterior de que el punto $x_n$ pertenezca al componente $k$ de la mezcla, mientras que $\\pi_k$ es el peso, o probabilidad a priori de la mezcla del componente $k$.\n",
    "\n",
    "De esta forma, suponiendo que $\\Sigma_k$ es no singular, la expresión anterior se puede reescribir como:\n",
    "$$\n",
    "\\mu_k = \\frac{1}{N_k} \\sum_{n=1}^N \\gamma(z_{nk}) x_n,\\\\\n",
    "N_k = \\sum_{n=1}^N \\gamma(z_{nk}),\n",
    "$$\n",
    "podemos interpretar $N_k$ como el número de puntos asignados al componente $k$ de la mezcla.\\\\\n",
    "\n",
    "De forma similar, para la matriz de covarianzas, se sigue un camino análogo derivando con respecto a $\\Sigma_k$, obteniendo:\n",
    "$$\n",
    "\\Sigma_k = \\frac{1}{N_k} \\sum_{n=1}^N \\gamma(z_{nk}) (x_n - \\mu_k)(x_n - \\mu_k)^\\top.\n",
    "$$\n",
    "Finalmente, para los pesos de la mezcla, por medio del lagrangiano, se obtiene:\n",
    "$$\n",
    "\\pi_k = \\frac{N_k}{N}.\n",
    "$$\n",
    "En el algoritmo EM, se alternan los pasos de Expectation (E) y Maximization (M) hasta que la convergencia sea alcanzada. En el paso E, se calcula la responsabilidad $\\gamma(z_{nk})$ para cada punto $x_n$ y componente $k$, mientras que en el paso M, se actualizan los parámetros del modelo (medias, covarianzas y pesos) utilizando las responsabilidades calculadas.\n",
    "\n",
    "A continuación, se presenta la implementación del algoritmo EM para ajustar una mezcla de $k$ distribuciones gaussianas a un conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c8b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixtureModel:\n",
    "    \"\"\"\n",
    "    A Gaussian Mixture Model (GMM) implementation using the Expectation-Maximization (EM) algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components, max_iters=100, tol=1e-4, random_state=None):\n",
    "        \"\"\"\n",
    "        Initialize the Gaussian Mixture Model.\n",
    "        Parameters:\n",
    "        - n_components: Number of Gaussian components in the mixture.\n",
    "        Optional parameters:\n",
    "        - max_iters: Maximum number of iterations for the EM algorithm.\n",
    "        - tol: Convergence tolerance for log-likelihood.\n",
    "        - random_state: Seed for random number generator for reproducibility.\n",
    "        \"\"\"\n",
    "        # Number of Gaussian components\n",
    "        self.k = n_components\n",
    "        # Maximum number of EM iterations\n",
    "        self.max_iters = max_iters\n",
    "        # Convergence tolerance for log-likelihood\n",
    "        self.tol = tol\n",
    "        # Random seed for reproducibility\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _initialize_parameters(self, X):\n",
    "        \"\"\"\n",
    "        Initialize the parameters of the GMM.\n",
    "        Randomly selects initial means from the data points and initializes covariances.\n",
    "        \"\"\"\n",
    "        # Number of samples and features\n",
    "        n_samples, n_features = X.shape\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        # Randomly choose initial means from data points\n",
    "        indices = np.random.choice(n_samples, self.k, replace=False)\n",
    "        self.means_ = X[indices]\n",
    "        # Initialize covariances to the sample covariance, regularized\n",
    "        self.covariances_ = np.array([np.cov(X.T) + 1e-6 * np.eye(n_features) for _ in range(self.k)])\n",
    "        # Initialize equal priors\n",
    "        self.prior_ = np.full(self.k, 1 / self.k)\n",
    "\n",
    "    def _e_step(self, X):\n",
    "        \"\"\"\n",
    "        Perform the Expectation step of the EM algorithm.\n",
    "        Computes the posterior probabilities (responsibilities) for each component given the data.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        # Posterior probabilities (responsibilities)\n",
    "        self.posterior_ = np.zeros((n_samples, self.k))\n",
    "\n",
    "        for i in range(self.k):\n",
    "            # Compute probability of each point under component i\n",
    "            rv = multivariate_normal(mean=self.means_[i], cov=self.covariances_[i])\n",
    "            self.posterior_[:, i] = self.prior_[i] * rv.pdf(X)\n",
    "\n",
    "        # Normalize responsibilities across components\n",
    "        self.posterior_ /= self.posterior_.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def _m_step(self, X):\n",
    "        \"\"\"\n",
    "        Perform the Maximization step of the EM algorithm.\n",
    "        Updates the parameters (means, covariances, and priors) based on the current responsibilities.\n",
    "        Args:\n",
    "            X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        # Effective number of points assigned to each component\n",
    "        Nk = self.posterior_.sum(axis=0)\n",
    "        # Update priors\n",
    "        self.prior_ = Nk / n_samples\n",
    "        # Update means\n",
    "        self.means_ = np.dot(self.posterior_.T, X) / Nk[:, np.newaxis]\n",
    "\n",
    "        self.covariances_ = []\n",
    "        for i in range(self.k):\n",
    "            # Compute weighted covariance for each component\n",
    "            diff = X - self.means_[i]\n",
    "            weighted_diff = self.posterior_[:, i][:, np.newaxis] * diff\n",
    "            cov = weighted_diff.T @ diff / Nk[i]\n",
    "            cov += 1e-6 * np.eye(n_features)  # regularization for numerical stability\n",
    "            self.covariances_.append(cov)\n",
    "        self.covariances_ = np.array(self.covariances_)\n",
    "\n",
    "    def _gaussian_log_likelihood(self, X):\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood of the data given the current model parameters.\n",
    "        Args:\n",
    "            X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        likelihood = np.zeros((n_samples, self.k))\n",
    "        for i in range(self.k):\n",
    "            # Compute likelihood of each point under each component\n",
    "            rv = multivariate_normal(mean=self.means_[i], cov=self.covariances_[i])\n",
    "            likelihood[:, i] = self.prior_[i] * rv.pdf(X)\n",
    "        # Return total log-likelihood\n",
    "        return np.sum(np.log(likelihood.sum(axis=1)))\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the Gaussian Mixture Model to the data using the EM algorithm.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "        \"\"\"\n",
    "        # Initialize parameters\n",
    "        self._initialize_parameters(X)\n",
    "        self.log_likelihood_ = -np.inf\n",
    "\n",
    "        for _ in range(self.max_iters):\n",
    "            # E-step: update responsibilities\n",
    "            self._e_step(X)\n",
    "            # M-step: update parameters\n",
    "            self._m_step(X)\n",
    "            # Compute log-likelihood\n",
    "            new_log_likelihood = self._gaussian_log_likelihood(X)\n",
    "            # Check for convergence\n",
    "            if abs(new_log_likelihood - self.log_likelihood_) < self.tol:\n",
    "                break\n",
    "            self.log_likelihood_ = new_log_likelihood\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict the posterior probabilities of each component for the given data.\n",
    "        Args:\n",
    "            X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "            Returns:\n",
    "            np.ndarray: Posterior probabilities of shape (n_samples, n_components).\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        probs = np.zeros((n_samples, self.k))\n",
    "        for i in range(self.k):\n",
    "            # Compute probability of each point under each component\n",
    "            rv = multivariate_normal(mean=self.means_[i], cov=self.covariances_[i])\n",
    "            probs[:, i] = self.prior_[i] * rv.pdf(X)\n",
    "        # Normalize to get probabilities\n",
    "        probs /= probs.sum(axis=1, keepdims=True)\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the component labels for the given data based on the highest posterior probability.\n",
    "        Args:\n",
    "            X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "        Returns:\n",
    "            np.ndarray: Predicted component labels of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        # Assign each point to the component with highest probability\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"\n",
    "        Fit the model to the data and predict the component labels.\n",
    "        Args:\n",
    "            X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "        Returns:\n",
    "            np.ndarray: Predicted component labels of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.predict(X)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Get the parameters of the GMM.\n",
    "        Returns:\n",
    "            dict: Dictionary containing means, covariances, and priors.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'means': self.means_,\n",
    "            'covariances': self.covariances_,\n",
    "            'priors': self.prior_\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c1c220",
   "metadata": {},
   "source": [
    "Definimos funciones auxiliares para convertir la imagen a datos, y la función inversa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert an image to a feature array for clustering\n",
    "# Each pixel is represented as a 3-dimensional vector (RGB)\n",
    "def image_to_features(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')  # Load image and ensure RGB format\n",
    "    img_np = np.array(img)  # Convert image to numpy array\n",
    "    n, m, c = img_np.shape  # Get image dimensions\n",
    "    features = img_np.reshape(-1, 3)  # Flatten image to (num_pixels, 3)\n",
    "    return features, (n, m)\n",
    "\n",
    "# Convert clustered features back to image format\n",
    "def features_to_image(features, shape):\n",
    "    n, m = shape  # Unpack original image shape\n",
    "    return features.reshape(n, m, 3).astype(np.uint8)  # Reshape and convert to uint8 for image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d45e0",
   "metadata": {},
   "source": [
    "Para los 4 valores de $k$, (2, 3, 5 y 10), se ejecuta el algoritmo de E-M, o de mezcla de Gaussianas (GMM) para colorear la imagen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa226b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of different numbers of gaussians to try\n",
    "K = [2, 3, 5, 10]\n",
    "\n",
    "for k in K:\n",
    "    # Load and transform the image into features (pixels as RGB vectors)\n",
    "    features, shape = image_to_features(\"Figures/foto.jpg\")\n",
    "\n",
    "    # Fit the Gaussian Mixture Model to the pixel features\n",
    "    em = GaussianMixtureModel(n_components=k, max_iters=30, tol=1e-4, random_state=2025)\n",
    "    labels = em.fit_predict(features)  # Cluster assignment for each pixel\n",
    "\n",
    "    parameters = em.get_parameters()  # Get learned means\n",
    "\n",
    "    output = np.zeros_like(features)  # Prepare array for reconstructed image\n",
    "\n",
    "    # Assign each pixel the mean color of its cluster\n",
    "    for i in range(shape[0] * shape[1]):\n",
    "        output[i] = parameters['means'][labels[i]]\n",
    "\n",
    "    # Inverse transform: reshape features back to image and save result\n",
    "    reconstructed_img = features_to_image(output, shape)\n",
    "    Image.fromarray(reconstructed_img).save(f\"gmm_{k}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3c08b3",
   "metadata": {},
   "source": [
    "Las imágenes resultantes se encuentran guardadas con el nombre 'gmm_x.png', donde x es el número de gaussianas utilizadas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
